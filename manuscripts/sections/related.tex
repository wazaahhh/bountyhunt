\section{Background}
\label{sec:related}

Software reliability is an age-old problem \cite{littlewood1973bayesian,adams1984textordfeminineoptimizing,littlewood1989predicting}. Early empirical work on software bug discovery dates back to the time of UNIX systems \cite{miller1990empirical}, and over the years, numerous models for vulnerability discovery have been developed (see \cite{avgerinos2014enhancing,zhao2016empirical} for some contemporary approaches). As early as in 1989, it was recognized that the time to achieve a given level of software reliability is inversely proportional to the desired failure frequency level \cite{adams1984textordfeminineoptimizing}. For example, in order to achieve a $10^{-9}$ probability of failure, a software routine should be tested  $10^{9}$ times. Actually, the random variable $P(T > t) = 1/t$ corresponds to the Zipf's law \cite{maillart2008empirical,saichev2009theory}, which diverges as the random variable sample increases (i.e., no statistical moment is defined). Therefore, one can conclude that there will always be software vulnerabilities to be discovered as long as enough resources and time can be provided to find them. This problem can also be seen from an entropy maximization perspective, which is good for evolution (e.g., in biology) but detrimental for software engineering. As explained in \cite{brady1999murphy}, given the evolutionary nature of software, new bugs can be found in a software program as long as we have changes in use perspective. The difficulty of bug hunting is therefore not about finding bugs {\it per se}, but rather about envisioning all possible use cases which would reveal a software defect (i.e., program crash) or an unintended behavior.\\

Software solutions have been developed to systematically detect software inconsistencies and thus potential bugs (e.g., Coverity, FindBugs, SLAM, Astree, to name a few). However, to date, no systematic algorithmic approach has been found to detect and remove bugs at a speed that would keep pace with software evolution and expansion. Thus, human intelligence is still considered as one of the most efficient ways to explore novel use case scenarios -- by manual code inspection or with the help of bug testing software -- in which a software may not behave in the intended way.\\

Management techniques and governance approaches have been developed to help software developers and security researchers in their review tasks, starting with pair programming \cite{hulkko2005multiple}. To protect against cyber-criminals, it is also fashionable to hire {\it ethical hackers}, who have a mindset similar to potential attackers, to probe the security of computer systems \cite{smith2002ethical,saleem2006ethical,bishop2007penetration}. The policy of full disclosure, originating from the hacking and open source communities, have played a significant role in software security by forcing software owners to acknowledge and fix vulnerabilities discovered and published by independent researchers \cite{arora2008optimal}. The full-disclosure model has evolved into responsible disclosure, a standard practice where the security researcher agrees to allow a period of time for the vulnerability to be patched before publishing the details of the uncovered flaw. In most of these successful human-driven approaches, there is a knowledge-sharing component, either between two programmers sitting together in front of a screen, ethical hackers hired to probe the weaknesses of a computer system, or the broader community being exposed to open source code and publicly disclosed software vulnerabilities. Thus, Eric Raymond's famous quote ``Given enough eyeballs, all bugs are shallow" \cite{raymond1999cathedral} tends to hold, even though in practice things are often slightly more complicated \cite{hafiz2015game}.\\

Vulnerability {\it markets} have emerged in recent years to facilitate the trading of bugs and vulnerabilities, and they provide market-driven economic incentives to support the transfer of knowledge from security researchers to software organizations \cite{camp2004pricing}. These two-sided markets offer the potential to simultaneously harness the wisdom of crowds and reveal the security level of organizations through a competitive incentive mechanism \cite{schechter2002buy}. Nonetheless, the efficiency of vulnerability markets has been questioned on both theoretical \cite{kannan2005market,mckinney2007vulnerability} and empirical grounds \cite{ransbotham2008markets,algarni2014software}.\\

Building on previous work by Schechter \cite{schechter2002buy}, Andy Ozment \cite{ozment2004bug} theorized that the most efficient mechanisms are not markets {\it per se}, but rather auction systems \cite{milgrom1982theory}. In a nutshell, the proposed (monopsonistic) auction mechanism implies an initial reward $R(t=t_0) = R_0$, which increases linearly with time. If a vulnerability is reported more than once, only the first reporter receives the reward. Therefore, security researchers have an incentive to submit a vulnerability early (before other researchers submit the same vulnerability), but not too early, so that they can maximize their payoff $R(t) = R_0 + \epsilon \times t$ with $\epsilon$ the linear growth factor, which is meant to compensate for the increasing difficulty of finding each new bug. However, setting the right incentive structure $\{R_0,\epsilon \}$ is non-trivial given uncertainties in the amount of work needed, the level of competition (e.g., the number of researchers enrolled) in the bug bounty program \cite{pandey2014assessment}, or the nature and likelihood of overlap between two submissions by different researchers.\\

Regardless of these theoretical considerations, bug bounty programs have emerged as a tool used by many software organizations, with a range of heterogeneous incentive schemes \cite{finifter2013empirical}. For instance, some bug bounty programs include no monetary rewards \cite{zhao2014exploratory}. Meanwhile, dedicated platforms have been launched to act as trusted third parties in charge of clearing transactions between organizations and security researchers. These platforms also assist organizations in the design and deployment of their own program. One of the leading platforms is HackerOne which runs public and private programs for organizations across a wide range of business sectors. A subset of the public programs award bounties. These programs report bounty awards on their company program pages on the HackerOne website. Previous research has investigated vulnerability trends, response and resolve behaviors, as well as reward structures of participating organizations \cite{zhao2014exploratory,zhao2015empirical}. In particular, it was found that a considerable number of organizations experienced diminishing trends for the number of reported vulnerabilities, even as the monetary incentives exhibit a significantly positive correlation with the number of vulnerabilities reported \cite{zhao2015empirical}.

%Achieving software reliability has concerned engineers for at least four decades \cite{littlewood1973bayesian,adams1984textordfeminineoptimizing,littlewood1989predicting}. Early empirical work on software bug discovery dates back to the time of UNIX systems \cite{miller1990empirical}, and over years, numbers of models for discovering vulnerabilities have been developed (see \cite{avgerinos2014enhancing,zhao2016empirical} for some of the most contemporary approaches). However, as early as in 1989, it was recognized that the time to achieve a given level of software reliability is inversely proportional to the desired failure frequency level \cite{adams1984textordfeminineoptimizing}. For example, in order to achieve a $10^{-9}$ probability of failure, a software routine should be tested  $10^{9}$ times. Actually, the random variable $P(T > t) = 1/t$ corresponds to the Zipf's law \cite{maillart2008empirical,saichev2009theory}, which diverges as the random variable sample increases (i.e., no statistical moment is defined), and thus, it was rightly concluded that there would be software vulnerabilities as long as enough resources and time could be provided to find them. This problem can also be seen from an entropy maximization perspective, which is good for evolution (e.g., in biology) but detrimental in software engineering. Concretely, as explained in \cite{brady1999murphy}, given the evolutionary nature of software, new bugs can be found in a software program as long as use perspectives change. The difficulty of bug hunting is therefore not about finding a bug {\it per se}, but rather about envisioning all possible use situations, which would reveal a software defect (i.e., program crash) or an unintended behavior.\\
%
%Software solutions have been developed to systematically detect software inconsistencies and thus potential bugs (e.g., Coverity, FindBugs, SLAM, Astree, to name a few). However, to date, no systematic algorithmic approach has been found to get rid of bugs at a speed that would allow following the general pace of software evolution and expansion. Thus, human intelligence is still considered as one of the most efficient ways to explore novel situations -- by manual code inspection or with the help of bug testing software -- in which a software may not behave in the intended way.\\
%
%Management techniques and governance approaches have been developed to help software developers and security researchers in their review tasks, starting with pair programming \cite{hulkko2005multiple}. To protect against cyber-criminals, it is also fashionable to hire {\it ethical hackers}, who have a mindset similar to potential attackers, in order to probe the security of computer systems \cite{smith2002ethical,saleem2006ethical,bishop2007penetration}. Inherited from the hacking and open source philosophies, the full disclosure policy has been hotly debated as promoting a safer Internet, by forcing software editors to recognize vulnerabilities discovered by independent researchers, and quickly fix them, as a result of publication on public forums \cite{arora2008optimal}. The full-disclosure model has evolved into responsible disclosure, a standard practice in which the security researcher agrees to allow a period of time for the vulnerability to be patched before publishing the details of the flaw uncovered. In most of these successful human-driven approaches, there is a knowledge-sharing component, may it be between two programmers sitting together in front of a screen, ethical hackers being hired to discover and explore the weaknesses of a computer system, or the broader community being exposed to open source code and publicly disclosed software vulnerabilities. Thus, Eric Raymond's famous quote ``Given enough eyeballs, all bugs are shallow" \cite{raymond1999cathedral}, tends to hold, even though in practice things are often slightly more complicated \cite{hafiz2015game}.\\
%
%Recognizing the need of human intelligence for tackling security bugs at scale, researchers have considered early on the importance of trading bugs and vulnerabilities as a valuable knowledge, often earned the hard way. Vulnerability {\it markets} have thus emerged as a way to ensure appropriate incentives for knowledge transfer from security researchers to software and Internet organizations \cite{camp2004pricing}, and in particular, to jointly harness the wisdom of crowds and reveal the security level of organizations through a competitive incentive scheme \cite{schechter2002buy}. The efficiency of vulnerability markets has however been nevertheless questioned on both theoretical \cite{kannan2005market,mckinney2007vulnerability} and empirical grounds \cite{ransbotham2008markets,algarni2014software}.\\
%
%Early on and building on previous work by Schechter \cite{schechter2002buy}, Andy Ozment \cite{ozment2004bug} recognized that in theory most efficient mechanism designs shall not be markets {\it per se}, but rather auction systems \cite{milgrom1982theory}. In a nutshell, the proposed (monopsonistic) auction mechanism implies an initial reward $R(t=t_0) = R_0$, which increases linearly with time. If a vulnerability is reported more than once, only the first reporter receives the reward. Therefore, security researchers have an incentive to submit a vulnerability early (before other researchers might submit the same vulnerability), but not too early, so that they can maximize their payoff $R(t) = R_0 + \epsilon \times t$ with $\epsilon$ the linear growth factor, which is also supposed to compensate for the increasing difficulty of finding each new bug. But setting the right incentive structure $\{R_0,\epsilon \}$ is not trivial, because it must account for uncertainties \cite{pandey2014assessment}, such as work needed, or effective competition (i.e., the number of researchers enrolled in the bug program). Furthermore, the probability of overlap between two submissions by different researchers has remained largely unknown.\\
%
%Regardless of theoretical considerations (or perhaps by integrating them), bug bounty programs have emerged as a tool used by specific software companies for their own needs and with rather heterogeneous incentive schemes \cite{finifter2013empirical}: For instance, some bug bounty programs may include no monetary reward \cite{zhao2014exploratory}. Meanwhile, dedicated platforms have been launched to act as trusted third parties in charge of clearing transactions between organizations and security researchers. These platforms also assist organizations in the design and deployment of their own program. One of the leading platforms is HackerOne which runs public and private programs for organizations across a wide range of business sectors. A subset of the public programs award bounties, which report bounty awards on their company program pages on the HackerOne website. Previous research has investigated vulnerability trends, response \& resolve behaviors, as well as reward structures of participating organizations \cite{zhao2014exploratory,zhao2015empirical}. In particular, it was found that a considerable number of organizations exhibit decreasing trends for the number of reported vulnerabilities, yet monetary incentives exhibit a significantly positive correlation with the number of vulnerabilities reported \cite{zhao2015empirical}.