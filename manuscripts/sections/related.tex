\section{Background}
\label{sec:related}
Achieving software reliability has concerned engineers for at least four decades \cite{littlewood1973bayesian,adams1984textordfeminineoptimizing,littlewood1989predicting}. Early empirical work on software bug discovery dates back to the time of UNIX systems \cite{miller1990empirical}, and over years, numbers of models for discovering vulnerabilities have been developed (see \cite{avgerinos2014enhancing,zhao2016empirical} for some of the most contemporary approaches). However, as early as in 1989, it was recognized that the time to achieve a given level of software reliability is inversely proportional to the desired failure frequency level \cite{adams1984textordfeminineoptimizing}. For example, in order to achieve a $10^{-9}$ probability of failure, a software routine should be tested  $10^{9}$ times. Actually, the random variable $P(T > t) = 1/t$ corresponds to the Zipf's law \cite{maillart2008empirical,saichev2009theory}, which diverges as the random variable sample increases (i.e., no statistical moment is defined), and thus, it was rightly concluded that there would be software vulnerabilities as long as enough resources and time could be provided to find them. This problem can also be seen from an entropy maximization perspective, which is good for evolution (e.g., in biology) but detrimental in software engineering. Concretely, as explained in \cite{brady1999murphy}, given the evolutionary nature of software, new bugs can be found in a software program as long as use perspectives change. The difficulty of bug hunting is therefore not about finding a bug {\it per se}, but rather about envisioning all possible use situations, which would reveal a software defect (i.e., program crash) or an unintended behavior.\\

Software solutions have been developed to systematically detect software inconsistencies and thus potential bugs (e.g., Coverity, FindBugs, SLAM, Astree, to name a few). However, to date, no systematic algorithmic approach has been found to get rid of bugs at a speed that would allow following the general pace of software evolution and expansion. Thus, human intelligence is still considered as one of the most efficient ways to explore novel situations -- by manual code inspection or with the help of bug testing software -- in which a software may not behave in the intended way.\\

Management techniques and governance approaches have been developed to help software developers and security researchers in their review tasks, starting with pair programming \cite{hulkko2005multiple}. To protect against cyber-criminals, it is also fashionable to hire {\it ethical hackers}, who have a mindset similar to potential attackers, in order to probe the security of computer systems \cite{smith2002ethical,saleem2006ethical,bishop2007penetration}. Inherited from the hacking and open source philosophies, the full disclosure policy has been hotly debated as promoting a safer Internet, by forcing software editors to recognize vulnerabilities discovered by independent researchers, and quickly fix them, as a result of publication on public forums \cite{arora2008optimal}. The full-disclosure model has evolved into responsible disclosure, a standard practice in which the security researcher agrees to allow a period of time for the vulnerability to be patched before publishing the details of the flaw uncovered. In most of these successful human-driven approaches, there is a knowledge-sharing component, may it be between two programmers sitting together in front of a screen, ethical hackers being hired to discover and explore the weaknesses of a computer system, or the broader community being exposed to open source code and publicly disclosed software vulnerabilities. Thus, Eric Raymond's famous quote ``Given enough eyeballs, all bugs are shallow" \cite{raymond1999cathedral}, tends to hold, even though in practice things are often slightly more complicated \cite{hafiz2015game}.\\

Recognizing the need of human intelligence for tackling security bugs at scale, researchers have considered early on the importance of trading bugs and vulnerabilities as a valuable knowledge, often earned the hard way. Vulnerability {\it markets} have thus emerged as a way to ensure appropriate incentives for knowledge transfer from security researchers to software and Internet organizations \cite{camp2004pricing}, and in particular, to jointly harness the wisdom of crowds and reveal the security level of organizations through a competitive incentive scheme \cite{schechter2002buy}. The efficiency of vulnerability markets has however been nevertheless questioned on both theoretical \cite{kannan2005market,mckinney2007vulnerability} and empirical grounds \cite{ransbotham2008markets,algarni2014software}.\\

Early on and building on previous work by Schechter \cite{schechter2002buy}, Andy Ozment \cite{ozment2004bug} recognized that in theory most efficient mechanism designs shall not be markets {\it per se}, but rather auction systems \cite{milgrom1982theory}. In a nutshell, the proposed (monopsonistic) auction mechanism implies an initial reward $R(t=t_0) = R_0$, which increases linearly with time. If a vulnerability is reported more than once, only the first reporter receives the reward. Therefore, security researchers have an incentive to submit a vulnerability early (before other researchers might submit the same vulnerability), but not too early, so that they can maximize their payoff $R(t) = R_0 + \epsilon \times t$ with $\epsilon$ the linear growth factor, which is also supposed to compensate for the increasing difficulty of finding each new bug. But setting the right incentive structure $\{R_0,\epsilon \}$ is not trivial, because it must account for uncertainties \cite{pandey2014assessment}, such as work needed, or effective competition (i.e., the number of researchers enrolled in the bug program). Furthermore, the probability of overlap between two submissions by different researchers has remained largely unknown.\\

Regardless of theoretical considerations (or perhaps by integrating them), bug bounty programs have emerged as a tool used by specific software companies for their own needs and with rather heterogeneous incentive schemes \cite{finifter2013empirical}: For instance, some bug bounty programs may include no monetary reward \cite{zhao2014exploratory}. Meanwhile, dedicated platforms have been launched to act as trusted third parties in charge of clearing transactions between organizations and security researchers. These platforms also assist organizations in the design and deployment of their own program. The currently leading platform is HackerOne \footnote{HackerOne, {\it https://hackerone.com/} (last access March, 4th 2016).}. HackerOne runs 35 public programs, for organizations across a wide range of business sectors, and for which bounty awards are reported on their website. Previous research has investigated vulnerability trends, response \& resolve behaviors, as well as reward structures of participating organizations. In particular, it was found that a considerable number of organizations exhibit decreasing trends for reported vulnerabilities, yet monetary incentives exhibit a significantly positive correlation with the number of vulnerabilities reported \cite{zhao2015empirical}.