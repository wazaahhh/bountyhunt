\section{Discussion and Recommendations}
\label{sec:discussion}

\subsection{Discussion}
Finding bugs in software code is one of the oldest and toughest problems in software engineering. While algorithm based approaches have been developed over the years, human verification has remained a prime way to debugging and vulnerability hunting. Moreover, the idea of getting ``enough eyeballs" to inspect the code has been a cornerstone argument for the open source software movement \cite{raymond1999cathedral} along with the full-disclosure argument, as well as for vulnerability markets \cite{bohme2006comparison}. Bug bounty programs are perhaps the latest successful incarnation of markets for trading bugs and vulnerabilities \cite{bohme2006comparison}, which set incentives to disclose early, combined with an increase of payoff for more rare and difficult bugs.\\

Here, we have found that the number of discovered bugs and vulnerabilities in a bounty program is super-linearly associated with the number of security researchers. However, the distribution of bugs found per researcher per program is skewed, but not extreme. In other words, each researcher enrolled in a bug bounty program may contribute her fair share of valid bugs, and no researcher is found to contribute orders of magnitude more than the average. This result is rather surprising, as the current common wisdom is rather to use bug bounty programs for selecting most talented security researchers \cite{moussouris2016}. In some way, there is a conceptual flaw in this common wisdom reasoning: If selecting one (resp. a few) particularly talented security researcher(s), then the Coase theorem would apply and it would be more interesting for an organization to internalize the resource by hiring security consultants, or having a in-house auditing team, both of which are already commonly done by organizations  \cite{coase1937}. And because bug bounty programs exist and develop, it somehow demonstrates that the former security practices are probably insufficient. On the contrary to this common wisdom, we posit that bug bounty programs reach a large and diverse population of security researchers, who can independently look at the focal software from as many different perspectives. \\

This observation is reminiscent of an early proposition on the topic: Brady et al.  \cite{brady1999murphy} took an evolutionary theory perspective to the problem of software reliability, and basically argued that software is sensitive to environmental changes (i.e., it is not evolutionary fit) because it is usually designed for one purpose. The purpose however changes over time (think e.g., of software packages in Linux, how they are surprisingly linked together  \cite{maillart2008empirical}, and how they are often used in unintended ways). On the contrary to species who adapt by the way of selection (only the fittest portion of the population survives), this feature is essentially absent in software, according to Brady et al.
%The distinction between biological and software system may be less relevant nowadays. Software evolution has come to resemble ever more biological systems: While not later than ten years ago, the forking practice was considered as a schism between unreconcilable views in a project community, often leading to 2 or more distinct communities with their own new software breed, nowadays forking has become a very coming practice on source code hosting platforms (e.g., GitHub). In essence, each forked code repository whether it allows software perform exactly the same task, or a slightly different task, adds to the stock of code (comparable to the stock of genes) highly desirable for resilience through evolutionary pressure. One may consider two slightly different programs evolved from the same root. At some point, a vulnerability (or a set of vulnerabilities) is found in the most popular one, which are not present in the less popular one. If these vulnerabilities cannot be overcome quickly, and assuming that both programs perform roughly the same tasks, the less popular will end up prevailing. Hence, the more forks, the better overall, event though one may find human cognitive biases (e.g., herding effect \cite{salganik2006experimental}, competing attention \cite{hansen2001competing}), which don't necessarily exist in nature.
Here, the focal point is a software piece, or more precisely a set of complementary software pieces, which define the service offered by the focal organization. Software fitness is assessed by security researchers, internally (internal audit), externally (ethical hackers), or by resorting to the crowd (bug bounty programs). The software runs in a well-defined environment, and it would be hard, if not impossible, to deploy it in other environments (i.e., for a different use, e.g., by a different population), in order to test its robustness. Note that some very large companies by a matter of fact extensively test their software in a variety of environments given the pervasive nature of their service. One may think of Facebook with currently more 1.5 billion users worldwide.\\

Because they all carry their own unique experience, security researchers offer a form of confrontation with environments, which are particularly adverse on purpose. Additionally, the more remote from the focal organization, the more original the view on the software piece (without the hassle of deployment). Our results offer a similar conceptual view, as well as with the quote by Eric Raymond ``Given enough eyeballs, all bugs are shallow". In sum, diversity of views prevails over accumulated expertise, although we make no claim that expertise is not required. We simply observe that its individual effects are just bounded. These results also cast questions on learning curves and incentives that keep the researcher dig for bugs in a program she is already familiar with. We observe that overall these incentives become quickly insufficient in comparison with the increasing difficulty for a researcher to find additional bugs.\\

Moreover, we find that the larger the population of enrolled researchers, the even more bugs are found. In that process, the initial windfall effect of a newly launched program is critical and determines an important portion of the bug discovery timeline, and accordingly researchers are ready to switch their attention towards newly launched programs, at the expense of older ones. These results have critical implications for software security: If we consider an arbitrary focal bug to be discovered, the chance that it will be discovered increases with the number of researchers. If half researchers interested in the security of the focal software are black hats, there is roughly 50\% chance that the focal bug will be discovered by a black hat. If the proportion of population types (white and black hats) compounded over time is uneven, then the probability of discovery falling in one of both categories changes accordingly. In other words, in order to be effective (statistically speaking), a bug bounty program must reach much more white hats, compared to the estimated amount of black hats interested in finding holes in the focal software piece.\\

Most security researchers however participate in multiple bug bounty programs, and when a new program is launched, they face the strategic choice of switching program. Our results show that researchers have a decreasing incentive to explore higher ranks within the same program (Figure \ref{fig:scalings_awards}B), while they have an increasing, yet marginally decreasing, incentive to explore multiple programs (Figure \ref{fig:scalings_awards}C). We further confirm these results with a simple regression model that researchers tend to switch when new programs are launched. This is a strong signal that researchers make rational choices in the bug hunting environment: They have high incentives to switch quickly to a new program and harvest as fast as possible many frequent bugs with little reward, rather than less frequent yet more endowed bugs, even though the reward structure of bug bounty programs seems to incentivize generously the reward of high rank (i.e., less probable) vulnerabilities (Figure \ref{fig:scalings_awards}A). This result raises questions on some hard limits associated with incentives associated with bug discovery by humans: The disincentive clearly stems from the difficulty for individual researchers to reach high ranks (i.e., $P_{k} \rightarrow 0$ when $k \rightarrow \infty$), not from the reward scheme. While at first sight this turnover of security researchers may look bad to a bug bounty program manager, it may in the end be beneficial provided that a renewal of security researchers is provided. The bug bounty platform must be designed carefully to ensure that a sufficient inflow of new security researchers are enrolled and scattered among all programs hosted on the platform, including older ones, in order to compensate for researchers switching to new programs.\\

The migration of researchers from one program to another is also related to researchers' specialization. Since vulnerability discovery is a difficult and also competitive activity, researchers typically need to have specialized knowledge and skills. Previous research revealed that there are at least two types of specialization in bug bounty: program-specific and vulnerability-specific~\cite{zhao2014exploratory,zhao2015empirical}. \textit{Program-specific} specialization contains the knowledge, experience and skill of finding vulnerabilities in websites and software products under one particular program. Since specialization is relatively unique to the program, a specialized researcher has less incentive to switch. \textit{Vulnerability-specific} specialization focuses on knowledge and skills of a particular type of vulnerability, which can exist in many different products. Therefore, for researchers with vulnerability-specific specialization, they have stronger incentive to try out their skills on different bug bounty programs. Incorporating researcher specialization into the models discussed in this work would be an interesting future work. \\

Using {\it rankings} provides handy insights on the processes governing the vulnerability discovery process, and to some extent, associated incentives. However, the rank is an arbitrary measure of time, which hardly accounts for the effort spent on researching bugs, as well as for discounting effects. For instance, if the time required to find a vulnerability increases with the rank, then the expected payoff shall be discounted accordingly. Other aspects enter the equation: While most submissions occur early on after the program launch, this is also the moment when an organization might be less prepared to respond to a large flow of tasks, which in turn may trigger priority queueing and contingent delays \cite{maillart2011quantification}. While some workaround may be envisioned, publicly available data currently limit some desirable investigations, involving timing and discounting effects.\\

\subsection{Recommendations}

Based on the results of this work, we make the following recommendations to today's bug bounty programs, and bug bounty platforms like HackerOne and BugCrowd.

Currently, bug bounty programs rarely increase their bounty level overtime. However, we recommend that a bug bounty program shall consider increasing rewards constantly to offset the fast decay of probability to find a vulnerability of higher rank, as we have discussed in Section~\ref{sec:diversify}. In addition, increasing bounty can also offset the impact of newly launched programs on attracting researchers away, as we observed in Section~\ref{ols}. Therefore, the adjustment of bounty level should take both the vulnerabilities found, and also statistics of newly launched programs into accounts. Constructing a pricing model based on empirical results obtained in this work would be an interesting and important future work.

We also suggests that both individual bug bounty programs and bug bounty platforms shall aim at constantly attracting more researchers, in light of the results in Section~\ref{sec:enrollment}. Previous work~\cite{zhao2015empirical} has made several suggestions, such as a first time bonus, to achieve this goal. We further suggest that current private bounty programs should gradually increasing their enrollment and eventually go public. Receiving no bugs in the private stage does not indicate that the system is secure. Rather, it is more likely that the system has not been fully tested by the crowd. In addition, bug bounty platforms should improve their researcher invitation/allocation mechanisms to encourage the flow of diverse researchers to different programs~\cite{zhao2016crowdsourced}.

\subsection{Summary}

In this study, we have considered the incentive mechanisms at the aggregate level. Managers however organize enrollment, set incentives and tackle the operational pipeline, involving submission reviews and payroll processing. All these aspects, which are unique to each program, may crowd in, or on the contrary crowd out, security researchers from bug bounty programs. In particular their willingness to participate will be affected, but also the amount of effort they are ready to throw in the search of vulnerabilities. It is the hope of the authors to get increasingly fine-grained insights in the future, to compare bug bounty programs, and thus establish benchmarks of most performing programs, in an environment driven by large deviation statistics, and incentives structures, which resemble the St-Petersburg paradox, a well-known puzzle for decision making in behavioral economics.
